import torch
import time

# çŸ©é˜µå¤§å°ä¿æŒ 6000 (è¶³ä»¥è®©æ˜¾å¡å‡ºåŠ›)
N = 6000
LOOPS = 20  # å¾ªç¯è·‘ 20 æ¬¡ï¼Œè®©æ—¶é—´è¶³å¤Ÿé•¿

print(f"ğŸï¸ å‡†å¤‡å¼€å§‹è€ä¹…æµ‹è¯• (æ¯è½®è·‘ {LOOPS} æ¬¡)...")
print(f"ğŸ“Š è¯·ç°åœ¨æ‰“å¼€ Termius çš„ btop å‡†å¤‡è§‚å¯Ÿï¼")
print("-" * 40)

# --- 1. CPU è€ä¹…è·‘ ---
print("ğŸ¢ CPU é€‰æ‰‹å‡†å¤‡... (3ç§’åå¼€å§‹ï¼Œè¯·ç›¯ç€ btop çš„ CPU åŒºåŸŸ)")
time.sleep(3)
print("ğŸ CPU å¼€å§‹èµ·è·‘ï¼(ä½ ä¼šçœ‹åˆ° CPU å ç”¨é£™å‡)")

t0 = time.time()
a_cpu = torch.randn(N, N, device="cpu")
b_cpu = torch.randn(N, N, device="cpu")

for i in range(LOOPS):
    c = torch.matmul(a_cpu, b_cpu)
    print(f"  CPU è·‘å®Œç¬¬ {i + 1}/{LOOPS} åœˆ...")

print(f"ğŸ›‘ CPU ä¼‘æ¯ã€‚è€—æ—¶: {time.time() - t0:.2f} ç§’")
print("-" * 40)

# --- 2. GPU è€ä¹…è·‘ ---
if torch.backends.mps.is_available():
    device = torch.device("mps")

    print("ğŸš€ GPU (M4) é€‰æ‰‹å‡†å¤‡... (3ç§’åå¼€å§‹ï¼Œè¯·ç›¯ç€ btop çš„ GPU/Proc åŒºåŸŸ)")
    time.sleep(3)
    print("ğŸ GPU å¼€å§‹èµ·è·‘ï¼(ä½ ä¼šçœ‹åˆ°è¿›åº¦æ¡åˆ·å¾—é£å¿«)")

    # é¢„å…ˆåŠ è½½æ•°æ®åˆ°æ˜¾å­˜ï¼Œæµ‹è¯•çº¯è®¡ç®—é€Ÿåº¦
    a_gpu = torch.randn(N, N, device=device)
    b_gpu = torch.randn(N, N, device=device)

    # é¢„çƒ­ä¸€æ¬¡
    torch.matmul(a_gpu, b_gpu)

    t0 = time.time()
    for i in range(LOOPS):
        c = torch.matmul(a_gpu, b_gpu)
        torch.mps.synchronize()  # å¼ºåˆ¶åŒæ­¥ï¼Œç¡®ä¿æ¯ä¸€æ­¥éƒ½ç®—å®Œäº†
        print(f"  ğŸš€ GPU è·‘å®Œç¬¬ {i + 1}/{LOOPS} åœˆ...")

    print(f"ğŸ›‘ GPU å®Œèµ›ã€‚è€—æ—¶: {time.time() - t0:.2f} ç§’")
else:
    print("âŒ æ²¡æ£€æµ‹åˆ° GPU")